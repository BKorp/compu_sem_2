{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "psNs8XYNiug5"
   },
   "outputs": [],
   "source": [
    "from nltk.tag import tnt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JTDNeWdSmNyP"
   },
   "outputs": [],
   "source": [
    "pmb_data = pd.read_csv('sem-pmb_4_0_0-gold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Q8q_ErC_mN6p"
   },
   "outputs": [],
   "source": [
    "def data_extract(data):\n",
    "    '''Returns dataset in correct format for the NLTK TnT tagger tool'''\n",
    "    df = data.groupby(\"sent_file\")\n",
    "    column_val = []\n",
    "    for k, v in df:\n",
    "        column_val.append(v)\n",
    "\n",
    "    sentences = []\n",
    "    semtags = []\n",
    "    for n in column_val:\n",
    "        for x, y in n.items():\n",
    "            if x == 'token':\n",
    "                sentences.append(' '.join(map(str, y)))\n",
    "        elif x == 'semtag':\n",
    "            semtags.append(' '.join(map(str, y)))\n",
    "    return sentences, semtags\n",
    "\n",
    "def combine(sent, tags):\n",
    "    combined = []\n",
    "    sent = sent.split()\n",
    "    tags = tags.split()\n",
    "    for x, y in zip(sent, tags):\n",
    "        combined.append((''.join(x), ''.join(y)))\n",
    "    return combined\n",
    "\n",
    "sents, semtags = data_extract(pmb_data)\n",
    "data = []\n",
    "for x, y in zip(sents, semtags):\n",
    "    data.append(combine(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iWxNtHBgPRNC"
   },
   "outputs": [],
   "source": [
    "'''Splitting dataset into 80% train and 20% test'''\n",
    "train_data, test_data = train_test_split(data, test_size=0.20, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1xFuZT88i5OX"
   },
   "outputs": [],
   "source": [
    "'''Setting up TnT tagger and training the train dataset'''\n",
    "tnt_tagger = tnt.TnT()\n",
    "tnt_tagger.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbcoSwO_jxp-",
    "outputId": "1d44ab31-9b75-4019-e990-15bce1bb678e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the test set: 0.8972\n"
     ]
    }
   ],
   "source": [
    "'''Calculating the accuracy score (calculated by built-in TnT function)'''\n",
    "acc = tnt_tagger.accuracy(test_data)\n",
    "print('Accuracy score on the test set: {}'.format(round(acc, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OvxdSCrHxR8Q"
   },
   "outputs": [],
   "source": [
    "def predict(test_set):\n",
    "    '''Returns predicted list with the sentences, where every sentence is split \n",
    "    pairs, consisting of the words and their predicted label tag'''\n",
    "    x = []\n",
    "    x_test = []\n",
    "    for sent in test_set:\n",
    "        for word_pair in sent:\n",
    "            x.append(word_pair[0])\n",
    "        x_test.append(' '.join(x))\n",
    "        x = []\n",
    "  \n",
    "    pred = []\n",
    "    for n in x_test:\n",
    "        n = n.split()\n",
    "        pred.append(tnt_tagger.tag(n))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ClNKFWqi3TVa"
   },
   "outputs": [],
   "source": [
    "predictions = predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6hxph-n136i3"
   },
   "outputs": [],
   "source": [
    "'''Storing predicted and gold labels separately'''\n",
    "label_pred = []\n",
    "label_gold = []\n",
    "for sent, pred in zip(test_data, predictions):\n",
    "    for s, p in zip(sent, pred):\n",
    "        label_gold.append(s[1])\n",
    "        label_pred.append(p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qf5z-bHP4x1n",
    "outputId": "52909947-7ba4-4dd4-ccf9-e1a8bb668b0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALT     0.9773    0.9149    0.9451        47\n",
      "         AND     0.9865    0.6952    0.8156       105\n",
      "         APX     1.0000    1.0000    1.0000        18\n",
      "         ART     1.0000    0.1818    0.3077        11\n",
      "         BOT     1.0000    1.0000    1.0000         2\n",
      "         BUT     1.0000    0.8421    0.9143        19\n",
      "         CLO     0.9615    0.6410    0.7692        39\n",
      "         COL     1.0000    1.0000    1.0000        27\n",
      "         CON     0.9763    0.8154    0.8886      1766\n",
      "         COO     0.8077    0.8077    0.8077        26\n",
      "         CTC     1.0000    0.2500    0.4000         4\n",
      "         DEF     0.9150    0.9819    0.9473      1601\n",
      "         DEG     0.8333    0.8333    0.8333        60\n",
      "         DIS     0.9596    0.8480    0.9003       980\n",
      "         DOM     0.4000    0.4000    0.4000        10\n",
      "         DOW     1.0000    1.0000    1.0000         5\n",
      "         DST     0.9623    1.0000    0.9808        51\n",
      "         EFS     1.0000    0.6250    0.7692         8\n",
      "         EMP     0.9333    0.9091    0.9211        77\n",
      "         ENS     0.8904    0.8309    0.8596       479\n",
      "         EPS     0.9352    0.8794    0.9064       738\n",
      "         EQU     1.0000    0.6522    0.7895        23\n",
      "         EXG     0.9659    0.7692    0.8564       221\n",
      "         EXS     0.8387    0.7751    0.8056       369\n",
      "         EXT     0.7273    0.5424    0.6214        59\n",
      "         FUT     1.0000    0.9231    0.9600        65\n",
      "         GEO     1.0000    0.3846    0.5556        26\n",
      "         GPE     0.9519    0.7333    0.8285       135\n",
      "         GPO     0.8276    0.6667    0.7385        36\n",
      "         GRE     1.0000    0.0000    0.0000         1\n",
      "         GRP     0.7000    0.8400    0.7636        25\n",
      "         HAP     1.0000    0.3333    0.5000         3\n",
      "         HAS     0.9728    0.9728    0.9728       367\n",
      "         IMP     1.0000    0.5000    0.6667         2\n",
      "         INT     0.9355    0.9206    0.9280        63\n",
      "         IST     0.9497    0.7575    0.8428       499\n",
      "         ITJ     1.0000    1.0000    1.0000         1\n",
      "         LES     1.0000    0.2500    0.4000         4\n",
      "         LIT     0.5714    0.1739    0.2667        23\n",
      "         MOR     0.9667    0.9062    0.9355        32\n",
      "         MOY     1.0000    1.0000    1.0000        10\n",
      "         NEC     0.8750    0.7778    0.8235        18\n",
      "         NIL     0.9952    0.9826    0.9889      2122\n",
      "         NOT     0.9947    0.9895    0.9921       190\n",
      "         NOW     0.9480    0.9480    0.9480       711\n",
      "         NTH     1.0000    0.0000    0.0000         5\n",
      "         ORD     1.0000    0.8750    0.9333         8\n",
      "         ORG     0.8182    0.2903    0.4286        31\n",
      "         PER     0.9964    0.8160    0.8972       674\n",
      "         PFT     0.9333    0.8750    0.9032        16\n",
      "         POS     1.0000    0.9583    0.9787        24\n",
      "         PRG     1.0000    0.7500    0.8571         4\n",
      "         PRO     0.9844    0.9896    0.9870      1150\n",
      "         PRX     1.0000    1.0000    1.0000       101\n",
      "         PST     0.9570    0.9351    0.9459       262\n",
      "         QUC     0.9667    0.8208    0.8878       212\n",
      "         QUE     0.9783    0.9906    0.9844       318\n",
      "         QUV     1.0000    0.9322    0.9649        59\n",
      "         REF     0.7000    0.8750    0.7778         8\n",
      "         REL     0.9750    0.9922    0.9835      1024\n",
      "         ROL     0.9821    0.7794    0.8690       281\n",
      "         SCO     1.0000    0.6250    0.7692         8\n",
      "         SUB     0.8065    0.9259    0.8621        27\n",
      "         TOP     1.0000    0.8889    0.9412        18\n",
      "         UOM     0.9832    0.9512    0.9669       123\n",
      "         Unk     0.0000    1.0000    0.0000         0\n",
      "         XCL     0.0000    1.0000    0.0000         0\n",
      "         YOC     1.0000    0.4872    0.6552        39\n",
      "\n",
      "    accuracy                         0.8972     15470\n",
      "   macro avg     0.9094    0.7649    0.7815     15470\n",
      "weighted avg     0.9582    0.8972    0.9232     15470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_gold, label_pred, digits=4, zero_division=True))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
